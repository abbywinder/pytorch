{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29a487a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as func\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54b5770",
   "metadata": {},
   "source": [
    "# Setting the Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a390c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c363706d",
   "metadata": {},
   "source": [
    "# Defining a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1d538a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a class for a feedforward neural network with one hidden layer.\n",
    "# fc1 (fully-connected layer)\n",
    "# define each layer using nn.Linear as this is the linearity part of the model, the activation func is the bit that adds non-linearity\n",
    "\n",
    "class SampleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_prob=0.2):\n",
    "        super(SampleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=input_size, out_features=hidden_size, bias=True)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size, bias=True)\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        # layer 1\n",
    "        x = self.fc1(x)\n",
    "        x = func.relu(x) # hidden layer activation function\n",
    "#         x = nn.Dropout(p=self.dropout_prob)\n",
    "        # layer 2\n",
    "        x = self.fc2(x)\n",
    "#         x = func.sigmoid(x) # output layer activation function if required\n",
    "        return x\n",
    "    \n",
    "    def fit(self, X, y, num_epochs=500, patience=10):\n",
    "        for epoch in range(num_epochs):\n",
    "            # Forward pass\n",
    "            output = self.forward(X)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = loss_function(output, y)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5f810eb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SampleNN(\n",
       "  (fc1): Linear(in_features=5, out_features=3, bias=True)\n",
       "  (fc2): Linear(in_features=3, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an instance of the SampleNN class.\n",
    "sample_model = SampleNN(5,3,1, dropout_prob=0.2)\n",
    "sample_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa482e0",
   "metadata": {},
   "source": [
    "#### Hyperparameters - initialisation\n",
    "\n",
    "Here is where we define:\n",
    "- The number of layers\n",
    "- Number of neurons in each layer\n",
    "- The activation functions of each layer\n",
    "- Whether a bias term is added (to prevent overfitting)\n",
    "- Dropout (on/off) and magnitude of dropout:\n",
    "This 'turns off' neurons in order to prevent overfitting and introduce further non-linearity. This takes it from being fully-connected to not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e03e453",
   "metadata": {},
   "source": [
    "### Activation functions for hidden layers\n",
    "\n",
    "#### ReLU (Rectified Linear Unit):\n",
    "- Formula: f(x) = max(0,x)\n",
    "- Pros: Fast to compute, reduces vanishing gradient problem, generally works well in practice for deep networks.\n",
    "- Cons: Can suffer from \"dead neurons\" where some neurons never activate and therefore never update\n",
    "<br>\n",
    "\n",
    "#### Leaky ReLU:\n",
    "- Formula: f(x) = x for x>0 and f(x) = ax for x<=0\n",
    "- Pros: Addresses the \"dead neuron\" problem of ReLU by allowing a small gradient for negative values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7603fed",
   "metadata": {},
   "source": [
    "### Activation functions for output layers\n",
    "\n",
    "#### Binary classification\n",
    "- Neurons: 1\n",
    "- Activation Function: Sigmoid (to squash output between 0 and 1, representing the probability of the positive class).\n",
    "- Example: Email spam detection (spam or not spam).\n",
    "<br>\n",
    "\n",
    "#### Multi-class classification (single label)\n",
    "- Neurons: Equal to the number of classes.\n",
    "- Activation Function: Softmax (to output a probability distribution over multiple classes).\n",
    "- Example: Handwritten digit recognition (0 to 9).\n",
    "<br>\n",
    "\n",
    "#### Multi-label classification\n",
    "- Neurons: Equal to the number of classes.\n",
    "- Activation Function: Sigmoid for each neuron (since each class prediction is treated as an independent binary classification).\n",
    "- Example: Image tagging (an image can have multiple tags like \"beach\", \"sunset\", \"people\").\n",
    "<br>\n",
    "\n",
    "#### Regression\n",
    "- Neurons: 1 (for single-output regression) or more (for multi-output regression).\n",
    "- Activation Function: Usually linear or none. However, if the regression output has a known bounded range, activation functions like sigmoid or tanh might be used to bound the output.\n",
    "- Example: Predicting house prices\n",
    "<br>\n",
    "\n",
    "#### Time-series forecasting\n",
    "- Neurons: Can vary depending on the forecasting horizon (e.g., next value, next 10 values).\n",
    "- Activation Function: Often linear, especially if the values can range widely. For bounded values, other activations might be used.\n",
    "- Example: Stock price prediction for the next week\n",
    "<br>\n",
    "\n",
    "#### Autoencoders (for dimensionality reduction, denoising, etc.)\n",
    "- Neurons: Varies based on the desired reduced dimensionality or the structure of the data.\n",
    "- Activation Function: Could be linear, sigmoid, tanh, etc., depending on the nature of the data and the specific use-case.\n",
    "- Example: Image denoising\n",
    "<br>\n",
    "\n",
    "#### Generative Models (like GANs)\n",
    "- Neurons: Varies based on the structure and size of the data being generated.\n",
    "- Activation Function: Can vary, but for image generation tasks, the tanh activation is commonly used for the generator's output layer.\n",
    "- Example: Generating art images\n",
    "<br>\n",
    "\n",
    "#### Sequence-to-Sequence Problems (like translation)\n",
    "- Neurons: Often depends on the vocabulary size of the target language (for tasks like translation) or other sequence length details.\n",
    "- Activation Function: Softmax, especially when predicting tokens from a vocabulary.\n",
    "- Example: Translating English to French"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b713855",
   "metadata": {},
   "source": [
    "# Optimizing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c33fd6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a loss function (MSE loss) and an optimizer (SGD) for the model.\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.SGD(sample_model.parameters(), lr=0.001, weight_decay=0.001, momentum=0.9) # here we add learning rate, momentum, dampening etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e458044d",
   "metadata": {},
   "source": [
    "#### Hyperparameters - optimisation\n",
    "\n",
    "- Optimiser:\n",
    "E.g. SGD, Adam\n",
    "<br>\n",
    "\n",
    "- Loss function:\n",
    "E.g. MSE, cross-entropy\n",
    "<br>\n",
    "\n",
    "- Learning rate:\n",
    "How quickly the model should descend, bearing in mind that a low learning rate will have a slow descent but will converge smoothly. (Although runs the risk of overfitting by tracking noise. This can be offset partially by introducing momentum). A high learning rate will be quick but may never converge or find a minimum loss, resulting in high bias.\n",
    "<br>\n",
    "- Momentum: \n",
    "Helps to find actual direction of gradient descent, smoothing out variations such as noise, saddle points or plateaus. Momentum is given as a fraction `(gamma/beta)` and used iteravely, updating with each epoch. The proportion of momentum to the `current gradient*learning rate` is given by the fraction. Using momentum helps propel the optimiser in the direction of previously accumulated gradients. <br>\n",
    "\n",
    "`velocity = gamma * velocity + learning_rate * gradient` <br>\n",
    "`new_parameters = old_parameters - velocity`\n",
    "<br>\n",
    "- Batch size:\n",
    "What proportion of training data samples are used in SGD (remember S in SGD is stochastic, which means computing derivatives of a proportion of the data at random - less computationally expensive with minimal trade off in accuracy)\n",
    "<br>\n",
    "\n",
    "- Weight decay (on/off) and weight (magnitude of penalty):\n",
    "L2 regularisation - This adds a penalty to the loss function to prevent overfitting - basically introducing a small bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dc354c",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "131059ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some sample data for training.\n",
    "input_data = torch.rand(10000, 5)  # 10000 samples, each with 5 features\n",
    "target_data = torch.rand(10000, 1)  # 10000 samples, each with 1 target\n",
    "\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(input_data, target_data, test_size=0.5)\n",
    "\n",
    "# use fit module of SampleNN class to train\n",
    "sample_model.fit(X_train, y_train, num_epochs=500, patience=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5093948",
   "metadata": {},
   "source": [
    "#### Hyperparameters\n",
    "\n",
    "- Requires grad:\n",
    "Initialising data points with a requires_grad hyperparameters to make sure they are optimisable during training using gradient descent\n",
    "<br>\n",
    "- Number of epochs:\n",
    "The amount of forwards/backwards passes the model should make\n",
    "<br>\n",
    "- Early stopping rules:\n",
    "If no improvement is made after x epochs (known as patience) of the same loss value, then stop training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463e553d",
   "metadata": {},
   "source": [
    "# Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "83199b74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.083524615"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5)\n",
    "\n",
    "# regression\n",
    "score = mean_squared_error(y_val.detach().numpy(), sample_model(X_val).detach().numpy())\n",
    "score\n",
    "\n",
    "# # classification\n",
    "# score = accuracy_score(y_val.detach().numpy(), sample_model(X_val).detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d1098b",
   "metadata": {},
   "source": [
    "`Choose hyperparameters on the validation set`<br>\n",
    "`Get final score on the test set`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafa0657",
   "metadata": {},
   "source": [
    "#### Hyperparameters - Validating and Testing\n",
    "\n",
    "- Cross-validation:\n",
    "Should we cross-validate or not?\n",
    "- Hyperparameter optimisation:\n",
    "Could we use bayesian optimisation libraries to find the optimal hyperparameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78476f5c",
   "metadata": {},
   "source": [
    "# Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cee45d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: tensor([[0.4907]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Create a new input tensor for prediction.\n",
    "new_input = torch.rand(1, 5)\n",
    "\n",
    "# Make a prediction using the trained model.\n",
    "prediction = sample_model(new_input)\n",
    "\n",
    "# Print the prediction.\n",
    "print(\"Prediction:\", prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2958cc2",
   "metadata": {},
   "source": [
    "# Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a228479a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(sample_model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b7db30cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SampleNN(\n",
       "  (fc1): Linear(in_features=5, out_features=3, bias=True)\n",
       "  (fc2): Linear(in_features=3, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the model\n",
    "model = sample_model.to(device)\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7257e01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5874, 0.1977, 0.0122, 0.9257, 0.5728]]) tensor([[0.5074]])\n"
     ]
    }
   ],
   "source": [
    "new_data = torch.rand(1, 5)  # 1 sample with 5 features\n",
    "\n",
    "# Convert the data to a PyTorch tensor\n",
    "input_tensor = torch.FloatTensor(new_data)\n",
    "\n",
    "# If you used a GPU during training, move the input tensor to the same device\n",
    "if torch.cuda.is_available():\n",
    "    input_tensor = input_tensor.to('cuda')\n",
    "\n",
    "# Get the model's predictions\n",
    "with torch.no_grad():  # Needs to not be tracked with PyTorch autograd - don't want this sample to have impact on weights\n",
    "    outputs = model(input_tensor)\n",
    "\n",
    "print(new_data, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a2d64f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
